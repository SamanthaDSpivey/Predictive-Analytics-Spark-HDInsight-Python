{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "## Exploring Data w/ Spark DataFrames API & Spark SQL\n\n# Import/load packages to be used\n # To explore data, must load it into programmatic data object such as DataFrame.\n    # If structure of data is known ahead of time, can explicitly specify schema for DataFrame.\n\nfrom pyspark.sql.types import *\n\nflightSchema = StructType([\n  StructField(\"DayofMonth\", IntegerType(), False),\n  StructField(\"DayOfWeek\", IntegerType(), False),\n  StructField(\"Carrier\", StringType(), False),\n  StructField(\"OriginAirportID\", IntegerType(), False),\n  StructField(\"DestAirportID\", IntegerType(), False),\n  StructField(\"DepDelay\", IntegerType(), False),\n  StructField(\"ArrDelay\", IntegerType(), False),\n])\n\n# import data records w/ flight details\n\nflights = spark.read.csv('wasb:///data/raw-flight-data.csv', schema=flightSchema, header=True)\nflights.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1543259570503_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-hdinsi.heo40qq4u2te5ngvb4fudm1bve.yx.internal.cloudapp.net:8088/proxy/application_1543259570503_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-hdinsi.heo40qq4u2te5ngvb4fudm1bve.yx.internal.cloudapp.net:30060/node/containerlogs/container_e02_1543259570503_0006_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n+----------+---------+-------+---------------+-------------+--------+--------+\n|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n+----------+---------+-------+---------------+-------------+--------+--------+\n|        19|        5|     DL|          11433|        13303|      -3|       1|\n|        19|        5|     DL|          14869|        12478|       0|      -8|\n|        19|        5|     DL|          14057|        14869|      -4|     -15|\n|        19|        5|     DL|          15016|        11433|      28|      24|\n|        19|        5|     DL|          11193|        12892|      -6|     -11|\n|        19|        5|     DL|          10397|        15016|      -1|     -19|\n|        19|        5|     DL|          15016|        10397|       0|      -1|\n|        19|        5|     DL|          10397|        14869|      15|      24|\n|        19|        5|     DL|          10397|        10423|      33|      34|\n|        19|        5|     DL|          11278|        10397|     323|     322|\n|        19|        5|     DL|          14107|        13487|      -7|     -13|\n|        19|        5|     DL|          11433|        11298|      22|      41|\n|        19|        5|     DL|          11298|        11433|      40|      20|\n|        19|        5|     DL|          11433|        12892|      -2|      -7|\n|        19|        5|     DL|          10397|        12451|      71|      75|\n|        19|        5|     DL|          12451|        10397|      75|      57|\n|        19|        5|     DL|          12953|        10397|      -1|      10|\n|        19|        5|     DL|          11433|        12953|      -3|     -10|\n|        19|        5|     DL|          10397|        14771|      31|      38|\n|        19|        5|     DL|          13204|        10397|       8|      25|\n+----------+---------+-------+---------------+-------------+--------+--------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "# Infer Data Schema\n # If structure of data source is unknown, can have Spark automatically infer schema.\n    # Example- Load data about airports w/out knowing schema\n    \nairports = spark.read.csv('wasb:///data/airports.csv', header=True, inferSchema=True)\nairports.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+-----------+-----+--------------------+\n|airport_id|       city|state|                name|\n+----------+-----------+-----+--------------------+\n|     10165|Adak Island|   AK|                Adak|\n|     10299|  Anchorage|   AK|Ted Stevens Ancho...|\n|     10304|      Aniak|   AK|       Aniak Airport|\n|     10754|     Barrow|   AK|Wiley Post/Will R...|\n|     10551|     Bethel|   AK|      Bethel Airport|\n|     10926|    Cordova|   AK|Merle K Mudhole S...|\n|     14709|  Deadhorse|   AK|   Deadhorse Airport|\n|     11336| Dillingham|   AK|  Dillingham Airport|\n|     11630|  Fairbanks|   AK|Fairbanks Interna...|\n|     11997|   Gustavus|   AK|    Gustavus Airport|\n|     12523|     Juneau|   AK|Juneau International|\n|     12819|  Ketchikan|   AK|Ketchikan Interna...|\n|     10245|King Salmon|   AK| King Salmon Airport|\n|     10170|     Kodiak|   AK|      Kodiak Airport|\n|     13970|   Kotzebue|   AK| Ralph Wien Memorial|\n|     13873|       Nome|   AK|        Nome Airport|\n|     14256| Petersburg|   AK|Petersburg James ...|\n|     14828|      Sitka|   AK|Sitka Rocky Gutie...|\n|     12807| St. Mary's|   AK|  St. Mary's Airport|\n|     11445|   Unalaska|   AK|    Unalaska Airport|\n+----------+-----------+-----+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "# Use DataFrame Methods\n # Spark DataFrames provide functions used to extract & manipulate data.\n    # Example- Use SELECT function to return a new DataFrame containing columns selected from an existing DataFrame.\n    \ncities = airports.select(\"city\", \"name\")\ncities.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+--------------------+\n|       city|                name|\n+-----------+--------------------+\n|Adak Island|                Adak|\n|  Anchorage|Ted Stevens Ancho...|\n|      Aniak|       Aniak Airport|\n|     Barrow|Wiley Post/Will R...|\n|     Bethel|      Bethel Airport|\n|    Cordova|Merle K Mudhole S...|\n|  Deadhorse|   Deadhorse Airport|\n| Dillingham|  Dillingham Airport|\n|  Fairbanks|Fairbanks Interna...|\n|   Gustavus|    Gustavus Airport|\n|     Juneau|Juneau International|\n|  Ketchikan|Ketchikan Interna...|\n|King Salmon| King Salmon Airport|\n|     Kodiak|      Kodiak Airport|\n|   Kotzebue| Ralph Wien Memorial|\n|       Nome|        Nome Airport|\n| Petersburg|Petersburg James ...|\n|      Sitka|Sitka Rocky Gutie...|\n| St. Mary's|  St. Mary's Airport|\n|   Unalaska|    Unalaska Airport|\n+-----------+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "# Combine Operations\n # Can combine functions in a single statement to perform multiple operations on DataFrame.\n    # Use join function to combine flight & airports DataFrames.\n     # Then use groupBy & count functions to return number of flights from each airport.\n\nflightsByOrigin = flights.join(airports, flights.OriginAirportID == airports.airport_id).groupBy(\"City\").count()\nflightsByOrigin.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------------+------+\n|             City| count|\n+-----------------+------+\n|          Phoenix| 90281|\n|            Omaha| 13537|\n|   Raleigh/Durham| 28436|\n|        Anchorage|  7777|\n|           Dallas| 19503|\n|          Oakland| 25503|\n|      San Antonio| 23090|\n|     Philadelphia| 47659|\n|       Louisville| 10953|\n|Dallas/Fort Worth|105024|\n|      Los Angeles|118684|\n|       Sacramento| 25193|\n|     Indianapolis| 18099|\n|        Cleveland| 25261|\n|        San Diego| 45783|\n|    San Francisco| 84675|\n|        Nashville| 34927|\n|    Oklahoma City| 13967|\n|          Detroit| 62879|\n|         Portland| 30640|\n+-----------------+------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "# Count Rows in DataFrame\n # Exploring data is key task when building predictive solutions\n    # Determining statistics that will help understand data before building predictive models.\n\nflights.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "2719418"}], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "# Determine Summary Statistics\n # Use describe function to return DataFrame containing count, mean, std dev, min, & max values for each numeric column.\n\nflights.describe().show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+-----------------+------------------+-------+------------------+------------------+-----------------+-----------------+\n|summary|       DayofMonth|         DayOfWeek|Carrier|   OriginAirportID|     DestAirportID|         DepDelay|         ArrDelay|\n+-------+-----------------+------------------+-------+------------------+------------------+-----------------+-----------------+\n|  count|          2719418|           2719418|2719418|           2719418|           2719418|          2691974|          2690385|\n|   mean|15.79747468024408|3.8983907586108497|   null| 12742.26441172339|12742.455345592329|10.53686662649788| 6.63768791455498|\n| stddev|8.799860168985367|1.9859881390373617|   null|1501.9729397025808|1501.9692528927785|36.09952806643081|38.64881489390021|\n|    min|                1|                 1|     9E|             10140|             10140|              -63|              -94|\n|    max|               31|                 7|     YV|             15376|             15376|             1863|             1845|\n+-------+-----------------+------------------+-------+------------------+------------------+-----------------+-----------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": "# Determine Presence of Duplicates\n # Clean data - detect & remove duplicates that could affect model\n    # Use dropDuplicates function to create new DataFrame w/ duplicates removed.\n     # Subtracting dropped from original values determines how many rows are duplicates of other rows.\n\n    \nflights.count() - flights.dropDuplicates().count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "22435"}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "# Identify Missing Values\n # Either remove rows containing missing data or replace missing values w/ suitable replacement.\n    # Use dropna function to create DataFrame w/ any rows containing data removed\n     # Can specify subset of columns & whether row should be removed in 'any' or 'all' values are missing.\n        # Use new DataFrame to determine how many rows contain missing values.\n\nflights.count() - flights.dropDuplicates().dropna(how=\"any\", subset=[\"ArrDelay\", \"DepDelay\"]).count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "46233"}], "metadata": {"collapsed": false}}, {"execution_count": 9, "cell_type": "code", "source": "# Clean Data\n # Data contains both duplicates & missing values\n    # Use fillna function to replace missing values w/ specified replacement value.\n      # Replace missing ArrDelay & DepDelay values w/ 0, assuming there was no/zero delay.\n    # Remove duplicate rows\n\ndata = flights.dropDuplicates().fillna(value = 0, subset=[\"ArrDelay\", \"DepDelay\"])\ndata.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "2696983"}], "metadata": {"collapsed": false}}, {"execution_count": 10, "cell_type": "code", "source": "# Re-check Summary Statistics\n # After removing duplicates & replacing missing values, distribution of data may have changed.\n    # This in turn could affect any predictive models created.\n\ndata.describe().show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+------------------+-----------------+-------+------------------+------------------+------------------+------------------+\n|summary|        DayofMonth|        DayOfWeek|Carrier|   OriginAirportID|     DestAirportID|          DepDelay|          ArrDelay|\n+-------+------------------+-----------------+-------+------------------+------------------+------------------+------------------+\n|  count|           2696983|          2696983|2696983|           2696983|           2696983|           2696983|           2696983|\n|   mean|15.798996508320593|3.900369412784582|   null|12742.459424846207| 12742.85937657004|10.531134234068217|6.6679285705545785|\n| stddev|  8.80126719913545|1.986458242170198|   null|1502.0359941370625|1501.9939589817984| 36.06172819056572|38.583861473580725|\n|    min|                 1|                1|     9E|             10140|             10140|               -63|               -94|\n|    max|                31|                7|     YV|             15376|             15376|              1863|              1845|\n+-------+------------------+-----------------+-------+------------------+------------------+------------------+------------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 11, "cell_type": "code", "source": "# Explore Relationships in Data\n # Use corr function to calculate a correlation value b/w -1 & 1, indicating the strength of correlation b/w two fields.\n    # Strong positive correlation (near 1) indicates that high values for one column are often found w/ high values of the other.\n    # Strong negative correlation (near -1) indicates that low values for one column are often found w/ high values for the other.\n    # Correlation near 0 indicates little apparent relationship b/w fields.\n\ndata.corr(\"DepDelay\", \"ArrDelay\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.9392630367706964"}], "metadata": {"collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "# Use Spark SQL\n # Can use DataFrame API directly to query data\n    # Can persist DataFrames as table & use Spark SQL to query them using SQL language.\n    # SQL is often more intuitive to use when querying tabular data structures.\n\ndata.createOrReplaceTempView(\"flightData\")\nspark.sql(\"SELECT DayOfWeek, AVG(ArrDelay) AS AvgDelay FROM flightData GROUP BY DayOfWeek ORDER BY DayOfWeek\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+------------------+\n|DayOfWeek|          AvgDelay|\n+---------+------------------+\n|        1| 7.077989660973244|\n|        2|  4.39237404158651|\n|        3| 7.234625279280266|\n|        4|10.775574715480056|\n|        5|  8.71110560964396|\n|        6|2.1437428120738304|\n|        7|  5.25403935972552|\n+---------+------------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 14, "cell_type": "code", "source": "# Use Inline SQL Magic\n # 'Magics' w/in Juptyer Notebooks enable inclusion of inline code & functionality.\n    # %%sql enables SQL queries & visualize results directly in notebook.\n\n%load_ext sql\n\n%%sql\nSELECT DepDelay, ArrDelay FROM flightData\n\n# Can change 'Table' visualization of results to 'Scatter' visualization to better see relationship b/w DepDelay & ArrDelay values.\n # Positive correlation b/w DepDelay & ArrDelay seems to be linear relationship, creating diagonal line structure of plottede points. ", "outputs": [{"output_type": "stream", "name": "stderr", "text": "invalid syntax (<stdin>, line 1)\n  File \"<stdin>\", line 1\n    SELECT DepDelay, ArrDelay FROM flightData\n                  ^\nSyntaxError: invalid syntax\n\n"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Querying Multiple Tables\n # Can create & query multiple temporary tables.\n    # Create a temporary table from 'airports' DataFrame.\n     # Use inline query to query it together w/ flights data.\n    \nairports.createOrReplaceTempView(\"airportData\")\n\n%%sql\nSELECT a.name, AVG(f.ArrDelay) AS avgdelay\nFROM flightData AS f JOIN airportData AS a\nON f.DestAirportID = a.airport_id\nGROUP BY a.name", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Change above visualization to 'Bar' chart o see average lateness (or earliness) of flights at all destinations.", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}